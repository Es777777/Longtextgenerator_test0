# 长文本生成的bench相关调研与试用

## 1.LongGenbench

### 1.1 LongGenbench的原理

传统的评估基本上多以一次问一个问题形式，LongGenBench 为了尝试对长文本的生成去做评估重新设计了输入格式，他们做的这个评估系统会将K个独立的问题（来自现有的数据集）按顺序拼接在系统提示词之后，形成一个很长的输入 Prompt （见原库Evalution/evaluate.py）要求模型必须在一个单一的、连贯的长回复中，按顺序依次回答这 $K$ 个问题 。最终解析模型的长回复，提取出对应每一个问题的答案，并将其与标准答案（Ground Truth）进行比对，以验证正确性 。为了保证结果的稳健性，这个过程会重复 T 次迭代，每次使用不同的问题集 。

**1.1.1 数据集设置**

LongGenBench 综合了来自不同领域的三个数据集：世界知识，算术能力与常识推理
来自 MMLU 的World Knowledge数据集衡量模型在 57 个不同类别中理解和推理的能力，以准确性为主要评估指标。
来自 GSM8K 的Arithmetic数据集通过 8000 个小学水平的数学应用题评估算术问题解决能力，以解题率为主要指标。
来自 CommonSenseQA的Commonsense Reasoning通过基于 ConceptNet 的多项选择题测试常识推理，以准确率作为评估指标。

**1.1.2 参数设置**

K：单次查询中包含的问题数量，决定了上下文的长度。实验中 $K$ 的取值范围通常在 20 到 90 之间，目的是让输出长度接近模型的最大 Token 限制 。

T ：迭代次数，用于计算平均表现 。

**1.1.3 评价指标**

LongGenBench 主要关注两个维度的指标：

准确率分布（Accuracy Distribution）：观察模型在回答第 1 个问题到第 $K$ 个问题时的准确率变化。通常会发现随着问题序号（Index）增加（即生成的文本变长），准确率会逐渐下降 。

性能下降幅度（Performance Degradation / $\Delta$）：
Baseline：模型在传统短文本模式下（一次回答一个问题）的准确率 。
LongGenBench Score：模型在长文本生成模式下的平均准确率。
Delta ($\Delta$)：两者之间的差值。差值越小，说明模型的长文本生成能力越强、越稳定 。

### 1.2 关于LongGenbench的思考

这种方式测试到模型在长文本生成过程中维持逻辑一致性（Consistency in logical flow）的能力 。前面的文本将被迫成为模型的上下文。

在此我认为存在第一个点是，拼接的问题之间不存在相互之间的逻辑约束与关联，那么如果说对这些问题做好比较精确的语义分割和滑窗（当然没有那么简单，因为至少要把系统提示词，基础的要求置于全局的提示词地位而把局部的问题逐一去解决，即使没有比较好的前后逻辑链条仍然可以通过拆分再结合去生成出一个相对来说比较满足准确度并且相当长的文本回答，因此其实用这一套去直接测LLM或许是有效的，但应对针对长文本做了一定的agent的优化或许就要失效？

同时原文认为，模型在长文本生成过程中，之前的推理步骤（无论正确与否）都会积累在上下文中，导致错误信息的积累，然而如果测试的不是单纯的LLM就似乎不那么合适，这某种意义上其实与自检机制存在一定的冲突，如果拥有一个完整的自检机制，能够利用已有的信息，会对这一方面造成较大的干扰，当然我认为，倘若保证数据集不以公开可查询数据的机制出现（利用网络搜索自检）或以其他形式与检查来源尽可能不重叠，似乎也具备验证的合理性，但就该bench自主构建的权威性和可推广性都会收到很大的影响。

### 1.3 关于LongGenbench的测试

本部分我期望通过对LongGenbench以上的一些问题去构造一个比较“压力测试‘的demo，期望对LongGenbench的评估方案去做到一个击穿的效果，同时也是借此为后续的长文本生成的优化机制去做一个测试，即使像LGB这样直接去做很多问题的拼接并不是常见的使用场景，但可以为我们去抓读入文本的拆分提供测试，目前该agent还在构思阶段，这一部分还会包含其测试结果与原生LLM数据的对比，目前我并不确定我所提出的漏洞是否是真实存在的问题，但这部分的测试我认为是有必要的。

该agent的架构我目前的构思是包含我所做的先前的调研部分的一些已经实现的机制，此后可以进行一定的更换？

输入的语义拆分->逻辑树结构构建？->数据分支修剪，切块->分给子agent或者尝试拆分输出后在作为整体的语句块输出，以小块去做RAG？->输出后去做逻辑检查，目前好像基本上是用LLM之类的想法才能对长文本去做检查，但是我觉得语义切块未必不好？这个地方要担心和我们最后的bench环节不产生冲突->形成完整的，具备逻辑性的输出。

参考：
https://arxiv.org/abs/2410.04199 
https://github.com/mozhu621/LongGenBench/ LongGenbench原文及其库

## 2.Hellobench

### 2.1 Hellobench的构造

**2.1.1 任务分类**
Hellobench基于 Bloom's Taxonomy（布鲁姆分类法） 将长文本生成能力划分为不同认知层级，并设计了对应的 5 类任务 ：

1. **Open-ended QA（开放式问答）：** 对应“记忆（Remember）”层级。
2. **Summarization（长摘要）：** 对应“理解（Understand）”层级。
3. **Chat（长对话）：** 对应“应用（Apply）”层级，模拟真实人机对话。
4. **Text Completion（文本补全）：** 对应“分析（Analyze）”层级，包括故事续写、风格迁移等。
5. **Heuristic Text Generation（启发式生成）：** 对应“创造（Create）”层级，如根据提示写剧本、写小说。

**2.1.2 数据集设置**

数据包含 647 个样本，覆盖 38 个子类别。所有数据均来自真实互联网用户场景（如 Quora, Reddit, WildChat），且经过筛选确保需要生成超过 1000 单词的长文本 。

相较于LongGenbench的问题拼接的手法，这个会更适合我们的想法，一个完善的长文本的生成的手段的创造，以及相关的行文流畅度等问题或许在面对实际的用户场景的时候会更有效

我翻看了一下数据集，有拆分成字典形式去打任务tag

id：样本唯一标识（如 chat\_002）
category：任务子类（如 report\_write、guide\_generation）貌似还是蛮全面的
instruction：给模型的指令（例如“write 10 different templates to report mann whitney u-test”） 
checklists：评估维度列表，用于人工或自动打分，常见维度包括：是否完全理解指令、是否充分详细、是否易懂、事实准确性、是否像人写、整体是否完美等 
formatted_checklists：结构化的评估条目，便于程序读取 
num_checklist：评估条目数量（通常为 6）
raw_text：原始指令文本 

单个需要长文本生成的任务相较于之前的LongGenbench会更加的难以作弊以及更合乎正常的使用场景

**2.1.3 评估方法：HelloEval**

人工评估耗时且主观所以原文提出了一种名为 **HelloEval** 的评估方法，旨在以低成本实现与人类评估的高度一致性 。

Checklist-based 机制： 不直接让模型打分，而是针对每个任务设计具体的检查清单（Checklists）（例如：是否包含了所有关键点？是否有幻觉？）。

1. **准备阶段：** 收集少量人类评分数据，利用线性回归（Linear Regression）计算出每个检查项（Checklist item）的**权重** 。
2. **执行阶段：** 使用 GPT-4o 作为裁判（LLM-as-a-Judge），根据检查清单对模型输出进行打勾/打叉，最后加权计算出总分 。

原文说HelloEval 与人类评分的相关性远高于传统的 ROUGE、BLEU 指标或其他 LLM 直接打分的方法 。

我认为这个评估的想法还是很值得关注的，可以作为我们评估机制的基石想法，拆分->产生打分项->LLM检查

### 2.2 原文值得关注的结论

一些经过长文本微调的模型（如 Suri-I-ORPO）虽然能写得更长，但内容会出现严重的重复和质量下降，我们可以针对去看一下。

模型的“长上下文理解能力”与“长文本生成能力”之间存在负相关。也就是说，擅长读长文的模型，不一定擅长写长文 ，或许要去做多agent的协作？

### 2.3 关于Hellobench的测试

这方面应该主要以部署试用为主，没有太多的其他问题和想法，等其他库的调研做完后会去测试补上

参考：
https://arxiv.org/abs/2409.16191
https://github.com/Quehry/HelloBench Hellobench原文及其库

## 3. LongBench-Write (来自 LongWriter 项目)

LongWriter也是去解决长文本生成问题的，他们就是自己去做了一个bench，这种模式或也可行？

### 3.1 LongBench-Write的构造

**3.1.1 测试指令的组成**
LongBench-Write 包含120 条包含60 条中文指令和 60 条英文指令 。精心挑选的用户写作指令，旨在模拟真实且多样化的应用场景，指令被划分为四个具体的输出长度要求区间，以测试不同量级的生成能力 ：0 - 500 字，500 - 2000 字，2000 - 4000 字，4000 字以上

同时为了保证多样性，指令覆盖了 7 种不同的写作类型 ：
文学与创意写作 (Literature and Creative Writing)
学术与专著 (Academic and Monograph)
科普 (Popular Science)
功能性写作 (Functional Writing)
新闻报道 (News Report)
社区论坛 (Community Forum)
教育与培训 (Education and Training)

**3.1.2 评估指标**

鉴于长文本评估的复杂性，LongBench-Write 设计多个评分的维度，由**长度得分 ($S\_l$)** 和 **质量得分 ($S\_q$)** 组成。

输出长度得分 ($S\_l$)用于衡量模型生成的字数是否符合用户的要求，采用分段线性函数计算

如果输出长度与要求完全一致，得 **100 分**。
如果输出太长（超过要求的 4 倍）或太短（少于要求的 1/3），分数会衰减至 0 。

考虑到“写得太短”通常比“写得太长”更不可接受，该公式对**过短**的输出设置了更严厉的扣分系数 。

输出质量得分 ($S\_q$)使用 GPT-4o 作为裁判的方式进行打分。为了避免长度影响质量判断，裁判被明确要求忽略长度，仅关注内容质量 。 评价包含 6 个维度（每个维度 1-5 分）：
1. 相关性 (Relevance)： 是否切题
2. 准确性 (Accuracy)： 是否有事实错误或幻觉
3. 连贯性 (Coherence)： 结构是否清晰，逻辑是否流畅
4. 清晰度 (Clarity)： 语言表达是否清晰易懂
5. 广度与深度 (Breadth and Depth)： 内容是否丰富、深刻
6. 阅读体验 (Reading Experience)： 是否引人入胜

最终得分为长度得分和质量得分的平均值

这个多维度得分去取平均或者加权平均数的想法还是蛮可以的，甚至系数可以是变化的以模拟现实中只要一方面不能很好满足需要就会导致生成的长文本难以使用

### 3.3 关于LongBench-Write的测试

值得作为一部分bench的参考，目前也是打算仅部署测试

参考：
https://arxiv.org/pdf/2408.07055
https://github.com/THUDM/LongWriter LongWriter原文及其库

## 4. ProxyQA

这个项目认为传统的长文本评估（如 ROUGE、BLEU）关注词汇重叠，无法反映内容的真实质量；而基于 LLM 的评分（如 GPT-4 打分）成本高且存在偏见（这个地方也很值得我们去思考，要不要使用LLM去做生成文本的评估，怎么保证其公平性）。 ProxyQA 的作者团队认为，如果一篇生成的长文质量很高（内容详实、覆盖面广），那么它应该包含足够的信息来回答与该主题相关的具体问题。因此，他们通过“能否从生成的文本中找到相关问题的答案”来量化评估长文的质量 ，某种意义上我认为这是以完成任务的结果导向去评估长文本的生成水平，这似乎也基于LLM的读取长文本能力显著强于其生成长文本的能力。

### 4.1 ProxyQA 的结构

ProxyQA 的评估过程包含三个主要步骤 ：

**4.1.1 构建问题**

元问题 (Meta-Question)：开放式的长文本生成指令（例如：“请详细介绍2008年北京奥运会的影响”）。

代理问题 (Proxy-Questions)： 针对元问题，设计一组具体的、事实性的短问题（例如：“2008年奥运会的主体育场叫什么？”“开幕式导演是谁？”）。这些问题代表了元问题所涵盖的关键知识点。

注：作者团队构建了一个包含不同领域（如传记、社会事件、技术等）的数据集，每个元问题对应约 10 个代理问题 。这个构建长文本生成指令的思路目前相对其他工作来说似乎是最精巧的

**4.1.2基于代理问题评分**

根据元问题生成长文本后，这个项目使用一个问答系统（QA Model，或者更强的 LLM 如 GPT-4），将生成的长文章作为上下文（Context），尝试回答那一组代理问题。计算代理问题的回答准确率（Accuracy）。如果生成的文章包含的信息能正确回答代理问题，则得分；反之不得分。最终得分为该篇文章能答对的代理问题的比例。

这个构造对元问题与代理问题之间的关系要求很高，数据集的构建相对来说是比较有难度的，这点可能要再考虑一下

### 4.2 ProxyQA 的价值和问题

可解释性强，如果得分低，可以具体知道是哪些关键信息缺失，会比较明确，也方便后续去调试我们的agent构建。

问题对构造困难，原因如上

评估体系可能不够全面，比较针对问答场景或介绍性文章？或将其他类型文本的需求量化？但这又存在困难

### 4.3 关于LongBench-Write的测试

也值得作为一部分bench的参考，目前也是打算仅部署测试

参考：
https://arxiv.org/abs/2401.15042
https://github.com/Namco0816/ProxyQA ProxyQA原文及其库

注：ELI5 (Explain Like I'm 5)，ROUGE，BLEU这几个比较早的方案应该来说只能提供一部分参考，可用性并不强，就没有去细做

现在面临的问题：

1. 要不要去构建测试agent
2. 是否每一个都需要测试，怎么去做这个测试，测试效果又怎么衡量
3. 如果使用自定义的bench，我们怎么去构建全面的数据集和完善的分类机制，完全基于现有的多个bench也并非不可考虑？构建这样的bench的主要工作其实就分为：评估机制设计，数据集构建，长文本生成指令构建以及将这几项综合到项目的工作



